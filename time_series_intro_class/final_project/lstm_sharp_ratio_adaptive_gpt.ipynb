{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#####################################\n",
    "# GBM-Based Synthetic Dataset (Same as before)\n",
    "#####################################\n",
    "class GBMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, length=5000, seq_len=50, S0=100, mu=0.05, sigma=0.2, dt=1/252):\n",
    "        self.seq_len = seq_len\n",
    "        total_length = length + seq_len\n",
    "        prices = torch.zeros(total_length)\n",
    "        prices[0] = S0\n",
    "        for t in range(1, total_length):\n",
    "            Z = torch.randn(1)\n",
    "            prices[t] = prices[t-1] * torch.exp((mu - 0.5 * sigma**2)*dt + sigma*math.sqrt(dt)*Z)\n",
    "        self.data = prices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.seq_len]\n",
    "        y = self.data[idx+self.seq_len]\n",
    "        return x.unsqueeze(-1), y.unsqueeze(-1)\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Two-Layer LSTM Model\n",
    "#####################################\n",
    "class TwoLayerLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=2, output_size=1):\n",
    "        super(TwoLayerLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last_out = out[:, -1, :]\n",
    "        preds = self.fc(last_out)\n",
    "        return preds\n",
    "\n",
    "#####################################\n",
    "# Sharpe Ratio Function\n",
    "#####################################\n",
    "def sharpe_ratio(returns):\n",
    "    mean_ret = returns.mean()\n",
    "    std_ret = returns.std()\n",
    "    if std_ret == 0:\n",
    "        return torch.tensor(0.0, device=returns.device)\n",
    "    return mean_ret / std_ret\n",
    "\n",
    "#####################################\n",
    "# Helper: Compute Gradient Norm\n",
    "#####################################\n",
    "def compute_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item()**2\n",
    "    return total_norm**0.5\n",
    "\n",
    "#####################################\n",
    "# Training Setup\n",
    "#####################################\n",
    "seq_len = 50\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_dataset = GBMDataset(length=5000, seq_len=seq_len, S0=100, mu=0.05, sigma=0.2, dt=1/252)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = TwoLayerLSTM(input_size=1, hidden_size=32, num_layers=2, output_size=1)\n",
    "criterion_mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "lambda_sr = 0.1  # initial guess\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, epochs+1):\n",
    "    for i, (features, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        preds = model(features)\n",
    "        mse_loss = criterion_mse(preds, targets)\n",
    "\n",
    "        # Compute returns based on last price in the input sequence\n",
    "        last_price = features[:, -1, 0]  # shape: (batch,)\n",
    "        predicted_returns = (preds.squeeze() - last_price) / last_price\n",
    "        sr = sharpe_ratio(predicted_returns)\n",
    "        \n",
    "        # We want to find gradient norms for MSE and SR separately\n",
    "        # 1) Grad norm for MSE\n",
    "        # Backprop MSE alone\n",
    "        optimizer.zero_grad()\n",
    "        mse_loss.backward(retain_graph=True)\n",
    "        gradnorm_mse = compute_grad_norm(model)\n",
    "        \n",
    "        # 2) Grad norm for SR\n",
    "        # Clear grads and backprop SR alone\n",
    "        # SR appears in loss as (-lambda_sr * SR), so just backprop SR alone (as if loss = SR)\n",
    "        optimizer.zero_grad()\n",
    "        sr.backward(retain_graph=True)\n",
    "        gradnorm_sr = compute_grad_norm(model)\n",
    "\n",
    "        # 3) Adjust lambda_sr so gradnorm(MSE) = lambda_sr * gradnorm(SR)\n",
    "        # Avoid division by zero\n",
    "        if gradnorm_sr > 1e-12:\n",
    "            new_lambda_sr = gradnorm_mse / gradnorm_sr\n",
    "        else:\n",
    "            new_lambda_sr = lambda_sr  # if sr grad is negligible, keep lambda_sr same\n",
    "\n",
    "        lambda_sr = new_lambda_sr\n",
    "\n",
    "        # 4) Now do final backward with combined loss = MSE - lambda_sr * SR\n",
    "        optimizer.zero_grad()\n",
    "        loss = mse_loss - lambda_sr * sr\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}], Batch [{i+1}] Loss: {loss.item():.4f}, lambda_sr: {lambda_sr:.4f}, gradnorm_mse: {gradnorm_mse:.4f}, gradnorm_sr: {gradnorm_sr:.4f}\")\n",
    "\n"
   ],
   "id": "2edd457fdb7d1f59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
